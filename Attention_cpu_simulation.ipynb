{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3lpxRAhXJJKmaygF1YB4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hereagain-Y/TCR_VAE/blob/main/Attention_cpu_simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g7XSmEah_VaN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getSampleIndex(sample):\n",
        "    sample_list = list( set(sample) )\n",
        "    sample_array = np.array(sample)\n",
        "    sample_index = []\n",
        "    for s in sample_list:\n",
        "        sample_index.append( np.where(sample_array==s)[0] )\n",
        "    return sample_list, sample_index"
      ],
      "metadata": {
        "id": "JVuoo_Lq_a0h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L = 16 # 512 node fully connected layer\n",
        "        self.D =  8# 128 node attention layer\n",
        "        self.K = 1\n",
        "\n",
        "        # get attention score  make it into 1 dim \n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.K),\n",
        "            nn.Tanh() # nn,SELU()\n",
        "           \n",
        "            \n",
        "        )\n",
        "        \n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L * self.K, 1), # 16,1 \n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self,sequence,sample,count):\n",
        "        A = self.attention(sequence)\n",
        "        count_col = count.reshape(-1,1)\n",
        "        A = A*count_col\n",
        "        newA= torch.clone(A)\n",
        "        sample_list, sample_index = getSampleIndex(sample)\n",
        "        sample_feature = []\n",
        "        for i in range(len(sample_list)):\n",
        "            seq_numbers = sample_index[i]\n",
        "            seqs_in_bag = sequence[seq_numbers]\n",
        "            attention_bag = A[seq_numbers]\n",
        "            attention_bag = torch.softmax(attention_bag,dim=0)\n",
        "            seq_feature = seqs_in_bag*attention_bag\n",
        "            sample_feature.append(seq_feature.sum(dim=0))\n",
        "            newA[seq_numbers] = attention_bag\n",
        "        sample_feature =torch.stack(sample_feature,dim=0)\n",
        "        \n",
        "        predictions = self.classifier(sample_feature) \n",
        "        Y_hat = torch.ge(predictions, 0.5).float() \n",
        "        return predictions,Y_hat, sample_list, newA"
      ],
      "metadata": {
        "id": "TDPEp2A6_cPV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function( label, prediction ):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(label, prediction, reduction='sum')\n",
        "    return reproduction_loss\n",
        "       \n",
        "def calculate_classification_error(Y_hat,Y):\n",
        "    Y = Y.float()\n",
        "    error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
        "\n",
        "    return error, Y_hat  "
      ],
      "metadata": {
        "id": "CJFwYog-_fAZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = False\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "n_feature = 16   \n",
        "model = Attention()"
      ],
      "metadata": {
        "id": "Cjz1I5Xm_hFu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gen_multi_list(amount, length):\n",
        "    seqs = np.random.default_rng()\n",
        "    return [seqs.random(length) for _ in range(amount)]\n",
        "\n",
        "sequences = gen_multi_list(1000,16) #50*10\n",
        "sequences = torch.from_numpy( np.array(sequences)).float()\n",
        "counts =[]\n",
        "for i in range(0,1000):\n",
        "    n=random.randint(1, 5)\n",
        "    counts.append(n)\n",
        "\n",
        "counts = torch.from_numpy( np.array( counts ) ).float()\n",
        "A = ['a','b','c','d','e','f','g','h','i','j']\n",
        "\n",
        "\n",
        "samples=list(itertools.chain.from_iterable(itertools.repeat(x, 10) for x in A))\n"
      ],
      "metadata": {
        "id": "ncwundhG_ic5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label =[]\n",
        "for i in range(0,10):\n",
        "    n=random.randint(0, 1)\n",
        "    label.append(n)\n",
        "labels = np.repeat(label,10)\n",
        "  \n",
        "\n",
        "sample_label_map = { i:j for (i,j) in zip(samples,labels) }\n",
        "label=torch.from_numpy(np.array( label ).reshape( (len(label),1) )).float()\n"
      ],
      "metadata": {
        "id": "L_AMtIEQ_kug"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)  \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "wFdshhH1_mYZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "for ite in range(epochs):\n",
        "    overall_loss =0\n",
        "    train_error = 0    \n",
        "    predictions,Y_hat,sample_list, attention_weights = model(sequences, samples, counts)\n",
        "    #get true sample label, for now, use binary\n",
        "    #label_true = [ sample_label_map[s] for s in sample_list ]\n",
        "    #column vectorat\n",
        "    #label_true = np.array( label_true ).reshape( (len(label_true),1) )\n",
        "    #label_true = torch.from_numpy( label_true ).float()\n",
        "    label_true = label\n",
        "    #Y_hat.shape 10*1\n",
        "    #print(label_true[:5])\n",
        "    #print(Y_hat[:5])\n",
        "    loss = loss_function(predictions, label_true)\n",
        "    overall_loss +=loss.item()\n",
        "    \n",
        "    error, predicted_label = calculate_classification_error(Y_hat, label_true)\n",
        "    train_error += error\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (ite % 1000 == 0):\n",
        "      print('Train Set, Epoch: {}, Loss: {:.4f},Error: {:.4f}, Accuracy: {:.2f}%'.format(ite+1, overall_loss,train_error,accuracy_score(label_true, Y_hat)*100))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF8vJz1U_oiD",
        "outputId": "981bf35f-bc42-44b9-f5a6-3e18b1a05352"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set, Epoch: 1, Loss: 6.7557,Error: 0.3000, Accuracy: 70.00%\n",
            "Train Set, Epoch: 1001, Loss: 1.6365,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 2001, Loss: 0.1769,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 3001, Loss: 0.0240,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 4001, Loss: 0.0066,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 5001, Loss: 0.0024,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 6001, Loss: 0.0010,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 7001, Loss: 0.0005,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 8001, Loss: 0.0003,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 9001, Loss: 0.0002,Error: 0.0000, Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_weights[:5]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaLlp9uS_u7a",
        "outputId": "60f986e1-655d-494e-aec5-fa5967bca65e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0892],\n",
            "        [0.0121],\n",
            "        [0.0892],\n",
            "        [0.0121],\n",
            "        [0.0328]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    }
  ]
}