{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd+j6ZLEqkuYwydL1FJDiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hereagain-Y/TCR_VAE/blob/main/Dataloader_of_bag_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sparsemax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEMIjs5SClNH",
        "outputId": "2c2518dc-5d02-4730-8c55-d8aad4c3aa3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sparsemax\n",
            "  Downloading sparsemax-0.1.9-py2.py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from Sparsemax) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->Sparsemax) (4.1.1)\n",
            "Installing collected packages: Sparsemax\n",
            "Successfully installed Sparsemax-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7wQ-CQKfCJf-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd \n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pkgutil import extend_path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from six.moves import xrange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "#\n",
        "from scipy.spatial import distance\n",
        "import random \n",
        "from scipy import spatial\n",
        "from scipy import stats\n",
        "from sparsemax import Sparsemax\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = False\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "n_feature = 16   \n"
      ],
      "metadata": {
        "id": "Yfu_aOoLCSSb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXFGkFRCCUww",
        "outputId": "f22a8e8f-f0c4-455c-f342-40fb9fa9f47d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self,h_dim=64*10*10, z_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            \n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1), #16\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1), # 12\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1), #10\n",
        "            nn.ReLU()\n",
        "        \n",
        "        )\n",
        "\n",
        "        \n",
        "        \n",
        "        # mean 64*5*5 =\n",
        "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
        "        # var \n",
        "        #self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        # for decoder layer \n",
        "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "   \n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, kernel_size=5, stride=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "    #def alignmentscore(self,x,y):\n",
        "    #    scores = pairwise2.align.localds(x,y,align_matrix,open=open_penalty,extend=gap_penalty)\n",
        "    #    score_align = scores[0].score\n",
        "    #    return score_align\n",
        "        \n",
        "    def latent(self,x,y):\n",
        "        l1 = self.encoder(x)# 32\n",
        "        l1 = l1.view(-1,64*10*10) # 32\n",
        "        l1 = self.fc1(l1) # latent\n",
        "\n",
        "        l2 = self.encoder(y)# 32\n",
        "        l2 = l2.view(-1,64*10*10) # 32\n",
        "        l2 = self.fc1(l2) \n",
        "        # use cos-simialrity\n",
        "        #latent_dist =np.sqrt( np.sum(np.square(l1 - l2)) )\n",
        "        \n",
        "        \n",
        "        return l1,l2\n",
        "\n",
        "    def forward(self, x,y):\n",
        "      # for dataset 1\n",
        "        h1= self.encoder(x)\n",
        "        h1 = h1.view(-1,64*10*10)\n",
        "        z1 = self.fc1(h1)\n",
        "        z1 = self.fc3(z1) # 64*10*10\n",
        "        z1 = z1.view(-1,64,10,10)\n",
        "        z1 = self.decoder(z1)\n",
        "      # for dat2   \n",
        "        h2= self.encoder(y)\n",
        "        h2 = h2.view(-1,64*10*10)\n",
        "        z2 =self.fc1(h2)\n",
        "        z2 = self.fc3(z2) # to h dim \n",
        "        z2 = z2.view(-1,64,10,10)\n",
        "        z2 = self.decoder(z2)\n",
        "        \n",
        "        # aligment \n",
        "        #s = self.alignmentscore(x,y)\n",
        "        # latent distance\n",
        "        l1,l2 = self.latent(x,y)\n",
        "        \n",
        "        \n",
        "        \n",
        "        return z1,z2,l1,l2\n",
        "vaemodel=torch.load('/content/drive/MyDrive/DL/FeatureModel/Newcnn_sigmoid_5w_echo_cat_train.apx', map_location=\"cpu\")"
      ],
      "metadata": {
        "id": "5k80XtbCCWbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AAs= ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "index_code = {}\n",
        "code_index = {}\n",
        "l_max = 20\n",
        "for i in range(len(AAs)):\n",
        "    index_code[i] = AAs[i]\n",
        "    code_index[ AAs[i] ] = i\n",
        "\n",
        "def oneHotEncode(seq, l_max=l_max, index_code=index_code, code_index=code_index):\n",
        "    n_amino = 20\n",
        "    matrix = np.zeros((l_max,n_amino)).astype(int)\n",
        "    for i in range(len(seq)):\n",
        "        matrix[ i , code_index[seq[i]] ] = 1\n",
        "    return matrix\n",
        "\n",
        "# pca encoded \n",
        "pca_index = pd.read_csv(\"/content/drive/My Drive/DL/VAE/AA_indexPCA.csv\")\n",
        "d=pca_index.set_index('Unnamed: 0').T.to_dict('list')\n",
        "\n",
        "\n",
        "# pca normalization\n",
        "data = d.items()\n",
        "list_dat = list(d.values())\n",
        "arr = np.array(list_dat)\n",
        "ex = np.array(arr)\n",
        "ex_norm = (ex-ex.min(axis=0))/(ex.max(axis=0)-ex.min(axis=0))\n",
        "\n",
        "AAs=np.array(list(d.keys()))\n",
        "new_pca = {}\n",
        "\n",
        "for i in np.arange(20):\n",
        "    new_pca[AAs[i]]=ex_norm[i]\n",
        "\n",
        "new_pca\n",
        "d= new_pca\n",
        "\n",
        "def AAindexEncoding(Seq):\n",
        "    length_seq=len(Seq)\n",
        "    global l_max\n",
        "    AAE=np.zeros([l_max,20])\n",
        "    if length_seq<l_max:\n",
        "        for amino in range(length_seq):\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA] # add PC value \n",
        "            \n",
        "        for amino in range(length_seq,l_max):\n",
        "            AAE[amino,]=np.zeros(20)\n",
        "    else: \n",
        "        for amino in range(length_seq): # zero padding\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA]\n",
        "        \n",
        "    #AAE=np.transpose(AAE.astype(np.float32)) # row as PC. and column as AA sequence \n",
        "    return AAE \n",
        "\n",
        "  \n",
        "def GetFeatures(file):\n",
        "    hot_encode=[]\n",
        "    for seq in file:\n",
        "        hot_encode.append(AAindexEncoding(seq))\n",
        "    hot_encode=np.array(hot_encode,dtype=np.float32)\n",
        "    result=np.array(hot_encode,dtype=np.float32)\n",
        "    return(result)"
      ],
      "metadata": {
        "id": "-vYFQJwDCbIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = r\"/content/drive/My Drive/DL/Lung_data/\"\n",
        "df_mango= pd.DataFrame()\n",
        "all_files = glob.glob(os.path.join(data_files , \"*Brain_Met.txt\"))\n",
        "brain =[]\n",
        "counts1=[]\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename,delimiter='\\t',header=None,names=['seq','count'])\n",
        "    df=df.iloc[1:,:]\n",
        "    df['length'] = [len(seq) for seq in df['seq']]\n",
        "    df= df[ df['length']<=20 ]\n",
        "    count=len(df['seq']) # number of seqs \n",
        "    brain.append(df)\n",
        "    counts1.append(count)\n",
        "brain_data= pd.concat(brain, axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "q8FSKfnnCeer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lung_file =glob.glob(os.path.join(data_files , \"*Lung.txt\"))\n",
        "lung=[]\n",
        "counts2=[]\n",
        "for filename in lung_file:\n",
        "    df = pd.read_csv(filename,delimiter='\\t',header=None,names=['seq','count'])\n",
        "    df=df.iloc[1:,:]\n",
        "    df['length'] = [len(seq) for seq in df['seq']]\n",
        "    df= df[ df['length']<=20 ]\n",
        "    count=len(df['seq']) \n",
        "    lung.append(df)\n",
        "    counts2.append(count)\n",
        "lung_data= pd.concat(lung, axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "RlOJMPsYCgRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_brain=list(brain_data['seq'])\n",
        "seq_lung=list(lung_data['seq'])\n",
        "AA_b1=GetFeatures(seq_brain)\n",
        "AA_b2=GetFeatures(seq_brain)\n",
        "AA_l1=GetFeatures(seq_lung)\n",
        "AA_l2=GetFeatures(seq_lung)\n",
        "\n",
        "train_loaderx= DataLoader(torch.from_numpy(AA_b1).float(), batch_size=1000, shuffle=False)\n",
        "train_loadery = DataLoader(torch.from_numpy(AA_b2).float(),batch_size=1000,shuffle=False)\n",
        "\n",
        "lung_loaderx= DataLoader(torch.from_numpy(AA_l1).float(), batch_size=1000, shuffle=False)\n",
        "lung_loadery = DataLoader(torch.from_numpy(AA_l2).float(),batch_size=1000,shuffle=False)"
      ],
      "metadata": {
        "id": "jdsFaohqCh7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent1=[]\n",
        "latent2=[]\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (x, y) in enumerate(zip(train_loaderx, train_loadery)):\n",
        "        x=x.view(len(x),1,20,20).to(device)\n",
        "        y=y.view(len(y),1,20,20).to(device)\n",
        "        x_hat,y_hat,l1,l2= vaemodel(x,y)\n",
        "        #dis = torch.nn.functional.pairwise_distance(l1, l2,2)\n",
        "\n",
        "        latent1.append(l1)\n",
        "        latent2.append(l2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('=========calculate distance=================')\n",
        "stacked_l1 = torch.cat(latent1)\n",
        "stack_l1 =stacked_l1.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "ZyhK6X3VCqvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent3=[]\n",
        "latent4=[]\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (x, y) in enumerate(zip(lung_loaderx, lung_loadery)):\n",
        "        x=x.view(len(x),1,20,20).to(device)\n",
        "        y=y.view(len(y),1,20,20).to(device)\n",
        "        x_hat,y_hat,l1,l2= vaemodel(x,y)\n",
        "        #dis = torch.nn.functional.pairwise_distance(l1, l2,2)\n",
        "        latent3.append(l1)\n",
        "        latent4.append(l2)\n",
        "stacked_l3 = torch.cat(latent3)\n",
        "\n",
        "stack_l3 =stacked_l3.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "E8XuNlPXCslM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences= np.concatenate((stack_l1,stack_l3),axis=0)"
      ],
      "metadata": {
        "id": "HifBqg83Cuc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "normal_seq=NormalizeData(sequences)\n",
        "#normal_seq = torch.from_numpy( np.array(normal_seq)).float()\n",
        "seqnumber=counts1+counts2"
      ],
      "metadata": {
        "id": "I-uU6sXwCw03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, repeat"
      ],
      "metadata": {
        "id": "qksxzNPNCyYy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A= np.arange(1,19)"
      ],
      "metadata": {
        "id": "HZLL2pwyC0cD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples=list(chain.from_iterable((repeat(item, cnt) for item, cnt in zip(A,seqnumber))))\n",
        "label=[0,1]\n",
        "labels=list(itertools.chain.from_iterable(itertools.repeat(x, 9) for x in label))\n",
        "sequence_dat=pd.DataFrame(normal_seq, columns = [str(1+n) for n in range(32)])\n",
        "sequence_dat.insert(loc=0, column='bags', value=samples)\n",
        "sequence_dat.to_csv('/content/drive/My Drive/DL/latentdata.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Lv5Q1ptfC45p",
        "outputId": "7f3c9551-aab8-402c-8952-2996e0a7e99b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-8f7351388190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseqnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msequence_dat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msequence_dat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bags'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seqnumber' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path,balance_sample_num):\n",
        "  dat = pd.read_csv(path+\"/latentdata.csv\",sep=',')\n",
        "  index, cnt =np.unique(dat['bags'],return_counts=True)\n",
        "  data_list=[]\n",
        "  for i in index:\n",
        "    data_list.append(dat.loc[dat['bags']==1,'1':].to_numpy().tolist())\n",
        "  seq_array = np.array(data_list)\n",
        "  label=[0,1]\n",
        "  labels=list(itertools.chain.from_iterable(itertools.repeat(x, balance_sample_num) for x in label))\n",
        "  label_array=np.array(labels)\n",
        "\n",
        "  data = torch.tensor(seq_array, dtype=torch.float32)\n",
        "  target = torch.tensor(label_array,dtype=torch.float32)\n",
        "  dataset_all = TensorDataset(data,target)\n",
        "\n",
        "\n",
        "  return data_list,dataset_all"
      ],
      "metadata": {
        "id": "dob1euMUDD0k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list, dataset_all=load_data(r'/content/drive/My Drive/DL',9)"
      ],
      "metadata": {
        "id": "bWYTbiTwQg_W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNBSxIL8QuOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset_all, batch_size= 5)"
      ],
      "metadata": {
        "id": "KQHDfnRrDG8y"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgoquo6QUrZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L =32\n",
        "        self.K = 1\n",
        "        self.fclayers=nn.Sequential(\n",
        "            nn.Linear(self.L, self.L),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(), # nn,SELU()\n",
        "            nn.Linear(self.L, self.L),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.L, self.L),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU()\n",
        "            \n",
        "        )\n",
        "        self.attention = nn.Linear(self.L,self.K)\n",
        "        self.sparseatt = Sparsemax(dim=1)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K,self.L) ,\n",
        "            nn.BatchNorm1d(self.L),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L * self.K, 1), \n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self,x):\n",
        "        out = self.fclayers(x) #32\n",
        "\n",
        "        attention = torch.squeeze(self.attention(out)) # 1 \n",
        "        attention = torch.transpose(attention,0,1)\n",
        "\n",
        "     # attention =self.sparseatt(attention)\n",
        "        attention =torch.softmax(attention,dim=1)\n",
        "      #attention_bag= torch.softmax(attention,dim=1) \n",
        "      \n",
        "        out = torch.transpose(out,0,1)\n",
        "        new_features = torch.bmm(torch.unsqueeze(attention, 1), out)\n",
        "        new_features = torch.squeeze(new_features)\n",
        "      \n",
        "        sample_feature =self.out(new_features)\n",
        "        predictions = self.classifier(sample_feature) \n",
        "        Y_hat = torch.ge(predictions, 0.5).float() \n",
        "        return predictions, attention\n",
        "\n",
        "\n",
        "     # attention = torch.transpose(attention,0,1)\n",
        "\n",
        "\n",
        "      # * original to get new futures \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "2W6OKpIbD2Jy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Attention()"
      ],
      "metadata": {
        "id": "aXTy_EFoQzbZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function( label, prediction ):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(label, prediction, reduction='sum')\n",
        "    return reproduction_loss\n",
        "       \n",
        "def calculate_classification_error(Y_hat,Y):\n",
        "    Y = Y.float()\n",
        "    error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
        "\n",
        "    return error, Y_hat  "
      ],
      "metadata": {
        "id": "mbEuEi_oQ9xC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "gO4E0aTkDH_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start train \n",
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)  \n",
        "from sklearn.metrics import accuracy_score\n",
        "epochs=1000"
      ],
      "metadata": {
        "id": "_pTbcRGkQ29f"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "der"
      ],
      "metadata": {
        "id": "L5u4lNAWFVv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, target in enumerate(train_loader):\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "p1Zmfoq7z1e-",
        "outputId": "89ba08c3-aa68-4ee8-e048-0287fb901e8c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-a74284e05762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (data,target) in enumerate(train_loader):"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "AtHnRILpE_c1",
        "outputId": "bfe27859-705b-4308-cb0c-6b849f09af34"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-6d2b5e24e820>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for batch_idx, (data,target) in enumerate(train_loader):\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "  data, target = data.to(device=device, dtype=torch.float32), target.to(device=device)"
      ],
      "metadata": {
        "id": "rNKjDFeOFN6h"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss= []\n",
        "for epoch in range(epochs):\n",
        "    overall_loss =0\n",
        "    train_error = 0 \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      data,target = torch.transpose(data,1,0).to(device),target.to(device) #[5]\n",
        "      optimizer.zero_grad()\n",
        "     \n",
        "      predicitons, attention = model(data)\n",
        "      predicitons =torch.squeeze(predicitons)#[5]\n",
        "      loss =loss_function(predicitons,target)\n",
        "      overall_loss += loss.item()\n",
        "      #print(overall_loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    train_loss.append(overall_loss/len(dataset_all))\n",
        "\n",
        "    if (epoch % 100 == 0):\n",
        "      print(\"\\tEpoch\", epoch , \"complete!\", \"\\tAverage Loss: \", train_loss[epoch])\n",
        "    \n",
        "\n",
        "   #[5,1]\n",
        "    #Y_hat.shape 10*1\n",
        "    #print(label_true[:5])\n",
        "    #print(Y_hat[:5])\n",
        "    #loss = loss_function(predicitons, target)\n",
        "    #overall_loss +=loss.item()\n",
        "    \n",
        "    #error, predicted_label = calculate_classification_error(Y_hat, label_true)\n",
        "    #train_error += error\n",
        "    \n",
        "    #loss.backward()\n",
        "  \n",
        "    #()if (epoch % 100 == 0):\n",
        "      #print('Train Set, Epoch: {}, Loss: {:.4f},Error: {:.4f}, Accuracy: {:.2f}%'.format(epoch+1, overall_loss,train_error,accuracy_score(label_true, Y_hat)*100))    \n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "7LHFOmvDRDqu",
        "outputId": "22be4ffa-ed35-4b39-fa56-2c430e219019"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch 0 complete! \tAverage Loss:  0.6942630343967013\n",
            "\tEpoch 100 complete! \tAverage Loss:  0.6947920984692044\n",
            "\tEpoch 200 complete! \tAverage Loss:  0.7006981505288018\n",
            "\tEpoch 300 complete! \tAverage Loss:  0.6927659511566162\n",
            "\tEpoch 400 complete! \tAverage Loss:  0.6935693820317587\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-4ec52ee870d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mpredicitons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mpredicitons\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicitons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicitons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-3b7b26f69750>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m      \u001b[0;31m# attention =self.sparseatt(attention)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;31m#attention_bag= torch.softmax(attention,dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}