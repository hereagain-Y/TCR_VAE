{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNiweD9B1Hv53cIN9op02Zp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hereagain-Y/TCR_VAE/blob/main/Simulation_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "csA2P9ODp_UG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getSampleIndex(sample):\n",
        "    sample_list = list( set(sample) )\n",
        "    sample_array = np.array(sample)\n",
        "    sample_index = []\n",
        "    for s in sample_list:\n",
        "        sample_index.append( np.where(sample_array==s)[0] )\n",
        "    return sample_list, sample_index\n"
      ],
      "metadata": {
        "id": "nGpF0D9IqNiK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionNetwork(nn.Module):\n",
        "    def __init__(self, n_input_features=16):\n",
        "        super(AttentionNetwork, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "        self.Tanh = nn.Tanh()\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        attention_weights = self.Tanh( self.linear(x) )\n",
        "        return attention_weights\n",
        "        \n",
        "class OutputNetwork(nn.Module):\n",
        "    def __init__(self, n_input_features=16, n_output_features=1):       \n",
        "        super(OutputNetwork, self).__init__()                \n",
        "        self.linear = nn.Linear(n_input_features, n_output_features)\n",
        "    \n",
        "    def forward(self, inputs):        \n",
        "        predictions = torch.sigmoid( self.linear(inputs) )\n",
        "        return predictions\n",
        "        \n",
        "class myNet(nn.Module):\n",
        "    def __init__( self, n_input_features=16, attention_network = AttentionNetwork(n_input_features=16), output_network = OutputNetwork(n_input_features=16) ):       \n",
        "        super(myNet, self).__init__()\n",
        "        self.n_input_features = n_input_features\n",
        "        self.attention_nn = attention_network       \n",
        "        self.output_nn = output_network\n",
        "        \n",
        "       \n",
        "    def forward(self, sequence, sample, count):\n",
        "        \"\"\" \n",
        "        \n",
        "        Parameters\n",
        "        n_sequences_per_bag: torch.Tensor\n",
        "            Number of sequences per bag as tensor of dtype torch.long and shape (n_samples,)\n",
        "        \n",
        "        Returns\n",
        "        ----------\n",
        "        predictions: torch.Tensor\n",
        "            Prediction for bags of shape (n_samples, n_outputs)\n",
        "        \"\"\"     \n",
        "        #sequence should be np.array, so is sample\n",
        "        # Calculate attention weights f() before softmax function for all bags in mb (shape: (d_k, 1))\n",
        "        attention_weights = self.attention_nn(sequence) # 1*N\n",
        "        #we multiply original attention weight by count\n",
        "        count_col_vec = count.reshape(-1,1) # N*1 \n",
        "        attention_weights = attention_weights * count_col_vec\n",
        "        sample_list, sample_index = getSampleIndex(sample)\n",
        "        sample_vec = []\n",
        "        attention_weights_after_softmax = torch.clone( attention_weights )\n",
        "        #print(attention_weights)\n",
        "        for i in range(len(sample_list)):\n",
        "            sample_index_this = sample_index[i]\n",
        "            seq_this = sequence[ sample_index_this ] # get all seqs for that person  n*16\n",
        "            #turn count from row vec to a column vec\n",
        "            attention_weights_this = attention_weights[ sample_index_this ] # get attentions for that person  ?? is a row or colunmn vector?\n",
        "            attention_weights_this = torch.softmax(attention_weights_this, dim=0)\n",
        "            #each element of sequence is a row vector, attention weight is a column vector (if it is the result from attention network)\n",
        "            sample_vec_this = seq_this * attention_weights_this #N*16 \n",
        "            #so we sum along column\n",
        "            sample_vec.append(sample_vec_this.sum(dim=0))\n",
        "            attention_weights_after_softmax[ sample_index_this ] = attention_weights_this\n",
        "        \n",
        "        sample_vec = torch.stack(sample_vec, dim=0)       \n",
        "        # Calculate predictions (shape (N, n_outputs))\n",
        "        predictions = self.output_nn(sample_vec)  \n",
        "        Y_hat = torch.ge(predictions, 0.5).float() # return 0,1 \n",
        "             \n",
        "        return predictions, Y_hat,sample_list, attention_weights_after_softmax\n",
        "    \n",
        "def loss_function( label, prediction ):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(label, prediction, reduction='sum')\n",
        "    return reproduction_loss\n",
        "\n",
        "    \n",
        "def calculate_classification_error(Y_hat,Y):\n",
        "    Y = Y.float()\n",
        "    error = 1. - Y_hat.eq(Y).cpu().float().mean().data\n",
        "\n",
        "    return error, Y_hat  \n"
      ],
      "metadata": {
        "id": "xzpwRhn0qPxq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = False\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "n_feature = 16   \n",
        "model = myNet(n_input_features=n_feature, attention_network = AttentionNetwork(n_input_features=n_feature), output_network = OutputNetwork(n_input_features=n_feature)).to(DEVICE)"
      ],
      "metadata": {
        "id": "xpPvKdbaqUC-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "oqtqC1D8qfYu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gen_multi_list(amount, length):\n",
        "    seqs = np.random.default_rng()\n",
        "    return [seqs.random(length) for _ in range(amount)]\n",
        "\n",
        "sequences = gen_multi_list(100,16) #10*10\n",
        "counts =[]\n",
        "for i in range(0,100):\n",
        "    n=random.randint(1, 5)\n",
        "    counts.append(n)\n"
      ],
      "metadata": {
        "id": "0S3xon57qg92"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = torch.from_numpy( np.array(sequences)).float()"
      ],
      "metadata": {
        "id": "-mWzTpEgqjXt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = torch.from_numpy( np.array( counts ) ).float()\n",
        "# person _index  10 people \n",
        "A = ['a','b','c','d','e','f','g','h','i','j']\n",
        "\n",
        "\n",
        "samples=list(itertools.chain.from_iterable(itertools.repeat(x, 10) for x in A))\n",
        "\n",
        "\n",
        "\n",
        "label =[]\n",
        "for i in range(0,10):\n",
        "    n=random.randint(0, 1)\n",
        "    label.append(n)\n",
        "labels = np.repeat(label,10)\n",
        "  \n",
        "\n",
        "sample_label_map = { i:j for (i,j) in zip(samples,labels) }\n"
      ],
      "metadata": {
        "id": "KouP5-Nrqu1d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_label_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI29jMNq14BM",
        "outputId": "2a49c53d-f753-49b6-ec78-b414be70505c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'b': 1,\n",
              " 'c': 1,\n",
              " 'd': 1,\n",
              " 'e': 1,\n",
              " 'f': 1,\n",
              " 'g': 1,\n",
              " 'h': 1,\n",
              " 'i': 0,\n",
              " 'j': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ooaQAwcmq1Ub"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "ytJZdnJUrqW9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "for ite in range(epochs):\n",
        "    overall_loss =0\n",
        "    train_error = 0    \n",
        "    predictions,Y_hat,sample_list, attention_weights = model(sequences, samples, counts)\n",
        "    #get true sample label, for now, use binary\n",
        "    label_true = [ sample_label_map[s] for s in sample_list ]\n",
        "    #column vector\n",
        "    label_true = np.array( label_true ).reshape( (len(label_true),1) )\n",
        "    label_true = torch.from_numpy( label_true ).float()\n",
        "    #Y_hat.shape 10*1\n",
        "    #print(label_true[:5])\n",
        "    #print(Y_hat[:5])\n",
        "    loss = loss_function(predictions, label_true)\n",
        "    overall_loss +=loss.item()\n",
        "    \n",
        "    error, predicted_label = calculate_classification_error(Y_hat, label_true)\n",
        "    train_error += error\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (ite % 1000 == 0):\n",
        "      print('Train Set, Epoch: {}, Loss: {:.4f},Error: {:.4f}, Accuracy: {:.2f}%'.format(ite+1, overall_loss,train_error,accuracy_score(label_true, Y_hat)*100))\n",
        "    \n",
        "    #print(ite,loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCfXHcZ5qcNp",
        "outputId": "d1e0b8a9-4a88-48c9-b659-936f23dc3d96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set, Epoch: 1, Loss: 7.6178,Error: 0.8000, Accuracy: 20.00%\n",
            "Train Set, Epoch: 1001, Loss: 2.9629,Error: 0.2000, Accuracy: 80.00%\n",
            "Train Set, Epoch: 2001, Loss: 0.8552,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 3001, Loss: 0.2604,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 4001, Loss: 0.0719,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 5001, Loss: 0.0345,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 6001, Loss: 0.0159,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 7001, Loss: 0.0077,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 8001, Loss: 0.0052,Error: 0.0000, Accuracy: 100.00%\n",
            "Train Set, Epoch: 9001, Loss: 0.0018,Error: 0.0000, Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9D7V4tTP1iKr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK9vAuks3oOQ",
        "outputId": "2428bb5a-3d27-4a69-d356-f9056557b6bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model, '/content/drive/My Drive/DL/Prediction_model/simulation_100seq_1w_echo_train.apx')"
      ],
      "metadata": {
        "id": "z4IEM6vjq_CQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on the model"
      ],
      "metadata": {
        "id": "ia_jKzz556h7"
      }
    }
  ]
}